{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Natural language processing?\n",
    "![What is NLP?](what-is-nlp.png)\n",
    "\n",
    "A subject that sits at the intersection of human language, artificial intelligence and computer science.\n",
    "\n",
    "Yann LeCunn:\n",
    "> \"NLP, or natural language processing, is a field of AI that focuses on the interaction between computers and humans through natural language, enabling machines to understand, interpret, and generate human language.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aim of this tutorial:\n",
    "\n",
    "Give people a high-level idea of how text in natural language form in any type of physical media like text document, voice etc. is interpreted and understood by computers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic problem with natural language processing: How make machine understand data?\n",
    "\n",
    "Let's say you have a robot assistant at home. \n",
    "- You said on Tuesday to robot assistant: *I like apples.*\n",
    "- Two days later, you ask the assistant: *Give me fruit juice.*\n",
    "\n",
    "![Make machine understand](make-machine-und.jpg)\n",
    "\n",
    "To process your request correctly, robot must understand:\n",
    "- \"like\" refers to affinity for an object.\n",
    "- Apple is a fruit :joy:\n",
    "\n",
    "While this task is easy for humans, for machines, they need to have a pre-processed understanding of what each of these words mean.  \n",
    "\n",
    "Intuitively, for this purpose, we need to have a <b>Corpus</b> of general data for machine to be trained on to understand language.\n",
    "\n",
    "#### Corpus \n",
    "\n",
    "The entire list of documents that we use for our model to train is known as the \"corpus\" of the said model.  \n",
    "\n",
    "For example, for large language model GPT-2, the predecessor of GPT-3, The training corpus is known to be a large and diverse collection of unannotated text from various sources, including books, articles, websites, forums, and other publicly available internet texts.\n",
    "\n",
    "For our example, we will take a tiny corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_corpus = [\n",
    "        \"<p>this is a good phone</p>\", \n",
    "        \"this is a bad_mobile\", \n",
    "        \"her cat is @a good cat\", \n",
    "        \"he has a bad temperament\", \n",
    "        \"these \\\"mobile\\\" phones are not good\",\n",
    "        \"mobile phone is fairly simple\"\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-processing:\n",
    "\n",
    "We need to perform a set of steps for the model to be able to help machine understand human langauge, we perform the following steps for the same:\n",
    "- Data cleaning\n",
    "- Tokenization\n",
    "- Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Data cleaning:\n",
    "\n",
    "Data cleaning is a basic step in pre-processing that involves the following sub-steps:\n",
    "1. <b>Remove stopwords and regular expressions</b>: Remove few words from the statement that dont add any extra value to understanding the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment if nltk is not installed\n",
    "# !pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run below command to download nltk stopwords\n",
    "#!python -m nltk.downloader stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "## uncomment code for downloading list from nltk.\n",
    "## from nltk.corpus import stopwords\n",
    "## english_stopword_list = stopwords.words(\"english\")\n",
    "\n",
    "english_stopword_list = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers', 'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n",
    "def remove_stopwords_from_statement(statement):\n",
    "    return \" \".join([i for i in statement.split() if i not in english_stopword_list])\n",
    "\n",
    "print(\"document corpus after stopword elimination applied to the data\")\n",
    "stopword_removed_document_corpus = [remove_stopwords_from_statement(i) for i in document_corpus]\n",
    "print(stopword_removed_document_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unwanted_chars(line):\n",
    "    # Email remove\n",
    "    line = re.sub(r'\\S*@\\S*\\s?', '', line)\n",
    "    # New line character remove\n",
    "    line = re.sub(r'\\s+', ' ', line)\n",
    "    # Single quote remove\n",
    "    line = re.sub(r\"\\'\", '', line)\n",
    "    # Double quote remove\n",
    "    line = re.sub(r\"\\\"\", '', line)\n",
    "    # Underscore remove\n",
    "    line = re.sub(r'_', ' ', line)\n",
    "    # Link remove\n",
    "    line = re.sub(r'http\\S*\\s?', '', line)\n",
    "    # HTML tag remove\n",
    "    line = re.sub(r'<.*?>', '', line)\n",
    "    return line\n",
    "print(\"document corpus after removing regex expressions from data\")\n",
    "sanitized_corpus = [remove_unwanted_chars(i) for i in stopword_removed_document_corpus]\n",
    "print(sanitized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Stemming vs Lemmatization:** \n",
    "\n",
    "- Stemming: Removes prefixes, suffixes and other afixes from word to obtain the base form of the word.\n",
    "*Example: converts \"fairly\" to \"fairli\"*\n",
    "\n",
    "- Lemmatization: Transforms a word to their base dictionary form.\n",
    "*Example: converts \"running\" to \"run\"*\n",
    "\n",
    "![Stemming vs Lemmatization](diff_stem_lemma.webp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "## Lemmatization preferred to stemming for NLP tasks, used stemming here because of lower latency \n",
    "stemmer = PorterStemmer()\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def stem_document(document):\n",
    "    stemmed_statement = \" \".join(stemmer.stem(word) for word in document.split())\n",
    "    return stemmed_statement\n",
    "\n",
    "def lemmatize_document(document):\n",
    "    lemmatized_statement = \" \".join(lemmatizer.lemmatize(word, pos='v') for word in document.split())\n",
    "\n",
    "stemmed_corpus = [stem_document(i) for i in sanitized_corpus]\n",
    "#lemmatized_corpus = [lemmatize_document(i) for i in sanitized_corpus]\n",
    "\n",
    "print(\"stemmed document corpus\")\n",
    "print(stemmed_corpus, end=\"\\n\")\n",
    "#print(\"lemmatized document corpus\")\n",
    "#print(lemmatized_corpus, end=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Tokenization:\n",
    "\n",
    "- Post data cleaning, we have a stemmed document corpus.\n",
    "- For machines to learn the **meaning** of words/phrases, we need to tokenize a given statement.\n",
    "\n",
    "We use simple `split` operation in python string(s) to tokenize our statements in document corpus.\n",
    "\n",
    "*To train large-scale deep learning models, we extract N-word tokenization, we split a given document for every N words or so, considering the size of our corpus, a simple 1 word tokenization should suffice.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_statement(statement):\n",
    "    return statement.split()\n",
    "tokenized_corpus = [tokenize_statement(i) for i in stemmed_corpus]\n",
    "print(\"Tokenized corpus: \", end=\"\")\n",
    "print(tokenized_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Encoding: \n",
    "\n",
    "- Post data cleaning and tokenization, we have a stemmed document corpus and data corpus of tokens.\n",
    "- Still, we cannot pass this information to the machines, we still to **convert text into number/vector** representation in such a way that we **preserve the context and relationship** between words and sentences, so that machine can now **understand the patterns**.\n",
    "\n",
    "There are various ways to encode words/tokens given a document corpus, in this notebook, we would explore:\n",
    "\n",
    "- Index-based encoding\n",
    "- Bag-Of-Words encoding\n",
    "- Tf-IDF encoding\n",
    "\n",
    "Later, we would understand the concept of **embeddings** which are **learnt** vector representations and which involves a **training** process usually based on deep learning, some common encoding techniques are:\n",
    "\n",
    "- Word2Vec Encoding\n",
    "- BERT Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. p1. Index-based encoding\n",
    "\n",
    "Based on the first occurence of the word/token we assign an index to the token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_based_encoded_corpus(token_corpus):\n",
    "    data_corpus = sorted(set([token for statement_token_collection in token_corpus for token in statement_token_collection]))\n",
    "    print(\"Data corpus\")\n",
    "    print(data_corpus, end = \"\\n\\n\")\n",
    "\n",
    "    ## +1 is added as 0 is reserved for stopword.\n",
    "    encoded_corpus = [[data_corpus.index(token)+1 for token in statement_token_collection] for statement_token_collection in token_corpus]\n",
    "    print(\"Final encoded corpus\")\n",
    "    return encoded_corpus\n",
    "    \n",
    "print(\"Stemmed corpus\")\n",
    "print(stemmed_corpus, end=\"\\n\\n\")\n",
    "print(index_based_encoded_corpus(tokenized_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. p2. BOW-based encoding\n",
    "\n",
    "For each statement, we create a vector representation as long as the number of words in the entire corpus/vocabulary.  \n",
    "\n",
    "Based on appearance/non-apperance of the word in the corpus OR the frequency, we assign to each index of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow_based_encoded_corpus(token_corpus):\n",
    "    data_corpus = sorted(set([token for statement_token_collection in token_corpus for token in statement_token_collection]))\n",
    "    encoded_corpus = [[1 if word in statement_token_collection else 0 for word in data_corpus] for statement_token_collection in token_corpus]\n",
    "    return encoded_corpus\n",
    "\n",
    "print(\"BOW-based encoded corpus\")\n",
    "print(bow_based_encoded_corpus(tokenized_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. p3. TF-IDF based encoding\n",
    "![Tf idf](tf-idf2.png)\n",
    "\n",
    "Similar to bag of words, we create a vector representation as long as the number of words in the entire corpus/vocabulary.\n",
    "\n",
    "Also, building on the concept of term frequency/appearance of said word in the statement, we introduce an inverse document frequency also to the equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def tf_idf_based_corpus(token_corpus):\n",
    "    data_corpus = sorted(set([token for statement in token_corpus for token in statement]))\n",
    "\n",
    "    print(\"Term frequency matrix is represented as\")\n",
    "    tf_matrix = [[statement.count(word)/len(statement) for word in data_corpus] for statement in token_corpus]\n",
    "    print(tf_matrix, end=\"\\n\\n\")\n",
    "\n",
    "    print(\"Inverse document frequency vector is represented as\")\n",
    "    idf_vector = [len(list(filter(lambda x: x is not None, [statement if word in statement else None for statement in token_corpus]))) for word in data_corpus]\n",
    "    print(idf_vector, end=\"\\n\\n\")\n",
    "\n",
    "    print(\"Tf-idf matrix representing the entire corpus is depicted as\")\n",
    "    doc_count = len(token_corpus)\n",
    "    N_idf_vector = [math.log(doc_count/k + 1) for k in idf_vector]\n",
    "    tf_idf_matrix = [[tf_matrix[i][j] * N_idf_vector[i] for j in range(0, len(data_corpus))] for i in range(0, doc_count)] \n",
    "    return tf_idf_matrix\n",
    "\n",
    "tf_idf_corpus = tf_idf_based_corpus(tokenized_corpus)\n",
    "print(tf_idf_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information retrieval\n",
    "\n",
    "One application of NLP here which could be \"information retrieval\" also known as search :) (tf-idf is the base algorithm for Azure cognitive search).\n",
    "\n",
    "Suppost the user wants to know about \"phone\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_most_relevant_documents(token, k=2):\n",
    "    ### get the index of the token.\n",
    "    data_corpus = sorted(set([token for statement in tokenized_corpus for token in statement]))\n",
    "    token_idx = data_corpus.index(token)\n",
    "\n",
    "    ## sort by score of token to get most relevant documents\n",
    "    en_tf_idf_corpus = [[idx, item] for idx, item in enumerate(tf_idf_corpus)]\n",
    "    sorted_tf_idf_corpus = sorted(en_tf_idf_corpus, key=lambda x: -x[1][token_idx])\n",
    "\n",
    "    ## get the k most relevant documents from the corpus\n",
    "    valid_idxs = [doc[0] for doc in sorted_tf_idf_corpus][0:k]\n",
    "    \n",
    "    ## remember original document is document corpus :)\n",
    "    rel_docs = [document_corpus[v] for v in valid_idxs]\n",
    "    return rel_docs\n",
    "\n",
    "retrieve_most_relevant_documents(\"phone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keyword extraction\n",
    "\n",
    "Another application of encoding is \"keyword extraction\" for each statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keyword_statement_corpus():\n",
    "    ### data corpus of the token.\n",
    "    data_corpus = sorted(set([token for statement in tokenized_corpus for token in statement]))\n",
    "\n",
    "    ## enumerated token corpus\n",
    "    en_tf_idf_corpus = [[[idx, item] for idx, item in enumerate(statement)] for statement in tf_idf_corpus]\n",
    "    # print(en_tf_idf_corpus)\n",
    "\n",
    "    ## filtered and sorted token corpus\n",
    "    fil_tf_idf_corpus = [sorted(list(filter(lambda x:x[1] > 0, en_statement)), key=lambda x:-x[1]) for en_statement in en_tf_idf_corpus]\n",
    "\n",
    "    ## keyword lookup.\n",
    "    keywords = [[data_corpus[ent[0]] for ent in statement] for statement in fil_tf_idf_corpus]\n",
    "    return keywords\n",
    "\n",
    "print(\"Given a document corpus\")\n",
    "print(document_corpus, end=\"\\n\\n\")\n",
    "\n",
    "print(\"These are the keywords\")\n",
    "print(extract_keyword_statement_corpus())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings: High-level overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### uncomment following code to run a custom model.\n",
    "\n",
    "'''from gensim.models import Word2Vec\n",
    "\n",
    "define training data\n",
    "sentences = [\n",
    "    [\"king\", \"queen\", \"ruled\", \"wisely\"],\n",
    "    [\"man\", \"woman\", \"strolled\", \"together\"],\n",
    "    [\"man\", \"bought\", \"orange\", \"apple\", \"juice\"],\n",
    "    [\"count\", \"numbers\", \"from\", \"one\", \"to\", \"ten\"],\n",
    "    [\"queen\", \"loved\", \"fresh\", \"orange\", \"juice\"],\n",
    "]\n",
    "\n",
    "train model\n",
    "model = Word2Vec(sentences, min_count=1, vector_size=10)\n",
    "summarize the loaded model\n",
    "print(model)\n",
    "\n",
    "summarize vocabulary\n",
    "words = list(model.wv.key_to_index)\n",
    "print(words)\n",
    "access vector for one word\n",
    "print(model.wv[\"king\"])\n",
    "\n",
    "save model\n",
    "model.save(\"model.bin\")\n",
    "load model\n",
    "new_model = Word2Vec.load(\"model.bin\")\n",
    "print(new_model)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"\"\"man -0.094386 0.43007 -0.17224 -0.45529 1.6447 0.40335 -0.37263 0.25071 -0.10588 0.10778 -0.10848 0.15181 -0.65396 0.55054 0.59591 -0.46278 0.11847 0.64448 -0.70948 0.23947 -0.82905 1.272 0.033021 0.2935 0.3911 -2.8094 -0.70745 0.4106 0.3894 -0.2913 2.6124 -0.34576 -0.16832 0.25154 0.31216 0.31639 0.12539 -0.012646 0.22297 -0.56585 -0.086264 0.62549 -0.0576 0.29375 0.66005 -0.53115 -0.48233 -0.97925 0.53135 -0.11725\n",
    "woman -0.18153 0.64827 -0.5821 -0.49451 1.5415 1.345 -0.43305 0.58059 0.35556 -0.25184 0.20254 -0.71643 0.3061 0.56127 0.83928 -0.38085 -0.90875 0.43326 -0.014436 0.23725 -0.53799 1.7773 -0.066433 0.69795 0.69291 -2.6739 -0.76805 0.33929 0.19695 -0.35245 2.292 -0.27411 -0.30169 0.00085286 0.16923 0.091433 -0.02361 0.036236 0.34488 -0.83947 -0.25174 0.42123 0.48616 0.022325 0.5576 -0.85223 -0.23073 -1.3138 0.48764 -0.10467\n",
    "king 0.50451 0.68607 -0.59517 -0.022801 0.60046 -0.13498 -0.08813 0.47377 -0.61798 -0.31012 -0.076666 1.493 -0.034189 -0.98173 0.68229 0.81722 -0.51874 -0.31503 -0.55809 0.66421 0.1961 -0.13495 -0.11476 -0.30344 0.41177 -2.223 -1.0756 -1.0783 -0.34354 0.33505 1.9927 -0.04234 -0.64319 0.71125 0.49159 0.16754 0.34344 -0.25663 -0.8523 0.1661 0.40102 1.1685 -1.0137 -0.21585 -0.15155 0.78321 -0.91241 -1.6106 -0.64426 -0.51042\n",
    "queen 0.37854 1.8233 -1.2648 -0.1043 0.35829 0.60029 -0.17538 0.83767 -0.056798 -0.75795 0.22681 0.98587 0.60587 -0.31419 0.28877 0.56013 -0.77456 0.071421 -0.5741 0.21342 0.57674 0.3868 -0.12574 0.28012 0.28135 -1.8053 -1.0421 -0.19255 -0.55375 -0.054526 1.5574 0.39296 -0.2475 0.34251 0.45365 0.16237 0.52464 -0.070272 -0.83744 -1.0326 0.45946 0.25302 -0.17837 -0.73398 -0.20025 0.2347 -0.56095 -2.2839 0.0092753 -0.60284\n",
    "boy -0.32345 0.23332 -0.20082 -0.52848 1.0926 0.62445 -0.99859 0.28085 0.088326 0.36919 0.32199 0.3499 0.067459 0.24211 0.92565 -0.32581 -0.99134 0.80767 -0.22845 0.40076 -0.8577 1.3836 0.056439 0.76561 0.3608 -2.0692 -0.46679 0.12359 0.35127 -0.77092 2.2064 -0.42605 -0.24279 0.3832 0.6069 0.62835 0.31825 -0.8851 0.38329 -1.146 -0.41949 0.2606 -0.6568 -0.11511 1.0591 -0.61148 0.32152 -1.3182 0.31744 0.02527\n",
    "girl -0.34471 0.69563 -0.78086 -0.58482 1.2263 1.2544 -0.76466 0.40575 0.18862 0.098834 0.32557 -0.31816 0.23869 0.33554 1.0592 -0.25266 -1.0308 0.70027 0.030457 0.53866 -0.30279 1.7515 0.3128 1.2103 0.41335 -1.9421 -0.93756 0.32453 0.52249 -0.86708 2.1258 -0.20377 -0.19061 0.37736 0.66205 0.50211 -0.12544 -0.83069 0.21155 -1.3091 -0.44973 -0.11648 0.033598 -0.68382 0.99419 -0.88262 0.56601 -1.327 0.37319 0.022389\n",
    "apple 0.52042 -0.8314 0.49961 1.2893 0.1151 0.057521 -1.3753 -0.97313 0.18346 0.47672 -0.15112 0.35532 0.25912 -0.77857 0.52181 0.47695 -1.4251 0.858 0.59821 -1.0903 0.33574 -0.60891 0.41742 0.21569 -0.07417 -0.5822 -0.4502 0.17253 0.16448 -0.38413 2.3283 -0.66682 -0.58181 0.74389 0.095015 -0.47865 -0.84591 0.38704 0.23693 -1.5523 0.64802 -0.16521 -1.4719 -0.16224 0.79857 0.97391 0.40027 -0.21912 -0.30938 0.26581\n",
    "orange -0.42783 0.43089 -0.50351 0.5776 0.097786 0.2608 -0.68767 -0.31936 -0.25337 -0.37255 -0.045907 -0.53688 0.97511 -0.44595 -0.50414 -0.086751 -1.0645 0.36625 -0.52428 -1.3413 -0.2391 -0.58808 0.56378 -0.062501 -1.7429 -0.88077 -0.27933 1.4705 0.50436 -0.69174 2.0018 0.26663 -0.85679 -0.18893 -0.021125 -0.055118 -0.50337 -0.67157 0.55502 -0.8009 0.10695 0.1459 -0.55588 -0.64971 0.22046 0.67415 -0.45119 -1.1462 0.16348 -0.62946\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "print(\"Loading Glove Model...\")\n",
    "glove_model = {}\n",
    "wordlist = corpus.splitlines()\n",
    "for line in wordlist:\n",
    "    split_line = line.split()\n",
    "    word = split_line[0]\n",
    "    embedding = np.array(split_line[1:], dtype=np.float64)\n",
    "    glove_model[word] = embedding\n",
    "\n",
    "print(f\"{len(glove_model)} words loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def similarity_between_words(a, b):\n",
    "    print(\n",
    "        f\"Similarity between {a} and {b} is {dot(glove_model[a], glove_model[b])/(norm(glove_model[a])*norm(glove_model[b]))}\"\n",
    "    )\n",
    "\n",
    "similarity_between_words(\"man\", \"boy\")\n",
    "similarity_between_words(\"man\", \"king\")\n",
    "similarity_between_words(\"man\", \"orange\")\n",
    "similarity_between_words(\"man\", \"woman\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Alert:\n",
    "\n",
    "Folks who might be interested in understanding GPT architecture may have encountered the term \"Byte-pair encoding\", while the process also converts text data into a compressed vector representation, that is **NOT** the only motivation of the encoding process above, GPT like BERT and other deep learning models, uses embedding based vector representation as the *semantic* vector representation of a given text input"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
